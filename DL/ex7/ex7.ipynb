{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25655c10",
   "metadata": {},
   "source": [
    "# Build the Banana network\n",
    "\n",
    "Now download the ex6.pdf (last week's exercise), let's build the banana model using pytorch.\n",
    "\n",
    "Ex2 (3) computes the outputs of the network for the first text from Exercise 1. In order to make the computation by\n",
    "hand feasible, use a smaller network with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3212a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w1 = torch.Tensor(\n",
    "            [[0, 1, -2, 1],\n",
    "            [2, 3, 0, -1],\n",
    "            [1, 0, -3, 0]]\n",
    "        )\n",
    "b1 = torch.Tensor([0, 0, 0])\n",
    "\n",
    "w2 = torch.Tensor(\n",
    "            [[0, -2, 1],\n",
    "            [2, 0, -1]]\n",
    "        )\n",
    "b2 = torch.Tensor([1, 1])\n",
    "\n",
    "w3 = torch.Tensor(\n",
    "            [-1, 1]\n",
    "        )\n",
    "b3 = torch.Tensor([1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f673b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Hint: How to initialize the weight?**\n",
    "\n",
    "Example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c16ce684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.weight Parameter containing:\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [10., 20., 30., 40.]], requires_grad=True)\n",
      "nn.bias Parameter containing:\n",
      "tensor([42., 84.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleExample(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Linear(in_features=4, out_features=2, bias=True)\n",
    "\n",
    "        w1 = torch.Tensor(\n",
    "            [[1, 2, 3, 4],\n",
    "             [10, 20, 30, 40]]\n",
    "        ) \n",
    "        b1 = torch.Tensor([42, 84])\n",
    "        self.nn.weight.data.copy_(w1)\n",
    "        self.nn.bias.data.copy_(b1)\n",
    "\n",
    "mdl = SimpleExample()\n",
    "for name, param in mdl.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31412344",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- Your banana network should have the name \"Banana\".\n",
    "- The Banana network has three layers.\n",
    "- For the first two layers, we use ReLU. The last layer, we use Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "631b7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "w1 = torch.Tensor(\n",
    "            [[0, 1, -2, 1],\n",
    "            [2, 3, 0, -1],\n",
    "            [1, 0, -3, 0]]\n",
    "        )\n",
    "b1 = torch.Tensor([0, 0, 0])\n",
    "\n",
    "w2 = torch.Tensor(\n",
    "            [[0, -2, 1],\n",
    "            [2, 0, -1]]\n",
    "        )\n",
    "b2 = torch.Tensor([1, 1])\n",
    "\n",
    "w3 = torch.Tensor(\n",
    "            [-1, 1]\n",
    "        )\n",
    "b3 = torch.Tensor([1])\n",
    "\n",
    "class Banana(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=4, out_features=3)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(in_features=2, out_features=1)\n",
    "        self.ac3 = nn.Sigmoid()\n",
    "\n",
    "        self.layer1.weight.data.copy_(w1)\n",
    "        self.layer1.bias.data.copy_(b1)\n",
    "        self.layer2.weight.data.copy_(w2)\n",
    "        self.layer2.bias.data.copy_(b2)\n",
    "        self.layer3.weight.data.copy_(w3)\n",
    "        self.layer3.bias.data.copy_(b3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn1out = self.ac1(self.layer1(x))\n",
    "        nn2out = self.ac2(self.layer2(nn1out))\n",
    "        nn3out = self.ac3(self.layer3(nn2out))\n",
    "        return nn3out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f781d306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight Parameter containing:\n",
      "tensor([[ 0.,  1., -2.,  1.],\n",
      "        [ 2.,  3.,  0., -1.],\n",
      "        [ 1.,  0., -3.,  0.]], requires_grad=True)\n",
      "layer1.bias Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "layer2.weight Parameter containing:\n",
      "tensor([[ 0., -2.,  1.],\n",
      "        [ 2.,  0., -1.]], requires_grad=True)\n",
      "layer2.bias Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "layer3.weight Parameter containing:\n",
      "tensor([[-1.,  1.]], requires_grad=True)\n",
      "layer3.bias Parameter containing:\n",
      "tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# After finishing your impl, run the following code, \n",
    "# check the parameter tensors are the same as the given weights\n",
    "banana = Banana()\n",
    "for name, param in banana.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705719a",
   "metadata": {},
   "source": [
    "Q: \n",
    "- Use bag-of-words to represent text1, text2, and text3\n",
    "- Input text1, text2, and text3 to your banana network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6828f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the code\n",
    "text1 = torch.Tensor([0, 0, 1, 0])\n",
    "text2 = torch.Tensor([1, 1, 0, 1])\n",
    "text3 = torch.Tensor([0, 0, 3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "436ec9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9933], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# get the out1, out2, out3. e.g., out1 = banana(text1)\n",
    "out1 = banana(text1)\n",
    "out2 = banana(text2)\n",
    "out3 = banana(text3)\n",
    "print(out1)\n",
    "print(out2)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f129a0",
   "metadata": {},
   "source": [
    "# Multi-class Classification \n",
    "\n",
    "Build a new neural net named \"BananaCLS\". \n",
    "- It has three layers, activation functions for the three layers are: \n",
    "    1. ReLU\n",
    "    2. ReLu\n",
    "    3. Softmax `nn.Softmax()`\n",
    "- For each layers, the weights should be initialized using the given weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd8ab442",
   "metadata": {},
   "outputs": [],
   "source": [
    "w11 = torch.Tensor(\n",
    "            [[0, 1, -2, 1],\n",
    "            [2, 3, 0, -1],\n",
    "            [1, 0, -3, 0]]\n",
    "        )\n",
    "b11 = torch.Tensor([0, 0, 0])\n",
    "\n",
    "w22 = torch.Tensor(\n",
    "            [[0, -2, 1],\n",
    "            [2, 0, -1]]\n",
    "        )\n",
    "b22 = torch.Tensor([1, 1])\n",
    "\n",
    "w33 = torch.Tensor(\n",
    "            [[1, 1],\n",
    "            [-1, 2],\n",
    "            [0, -1]]\n",
    "        )\n",
    "b33 = torch.Tensor([0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2ee7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build you BananCLS network\n",
    "class BananaCLS(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=4, out_features=3)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.ac2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(in_features=2, out_features=3)\n",
    "        self.ac3 = nn.Softmax()\n",
    "\n",
    "        self.layer1.weight.data.copy_(w11)\n",
    "        self.layer1.bias.data.copy_(b11)\n",
    "        self.layer2.weight.data.copy_(w22)\n",
    "        self.layer2.bias.data.copy_(b22)\n",
    "        self.layer3.weight.data.copy_(w33)\n",
    "        self.layer3.bias.data.copy_(b33)\n",
    "    def forward(self, x):\n",
    "        nn1out = self.ac1(self.layer1(x))\n",
    "        nn2out = self.ac2(self.layer2(nn1out))\n",
    "        nn3out = self.ac3(self.layer3(nn2out))\n",
    "        return nn3out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a85782e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight Parameter containing:\n",
      "tensor([[ 0.,  1., -2.,  1.],\n",
      "        [ 2.,  3.,  0., -1.],\n",
      "        [ 1.,  0., -3.,  0.]], requires_grad=True)\n",
      "layer1.bias Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "layer2.weight Parameter containing:\n",
      "tensor([[ 0., -2.,  1.],\n",
      "        [ 2.,  0., -1.]], requires_grad=True)\n",
      "layer2.bias Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "layer3.weight Parameter containing:\n",
      "tensor([[ 1.,  1.],\n",
      "        [-1.,  2.],\n",
      "        [ 0., -1.]], requires_grad=True)\n",
      "layer3.bias Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# After building your class\n",
    "# Print the name and params of your network\n",
    "banana_cls = BananaCLS()\n",
    "for name, param in banana_cls.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbaeb09",
   "metadata": {},
   "source": [
    "- Run the forward pass for text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5896dfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7054, 0.2595, 0.0351], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run the forward pass for text 1\n",
    "out_cls1 = banana_cls(text1)\n",
    "print(out_cls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72db4d",
   "metadata": {},
   "source": [
    "# Other questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7abd4",
   "metadata": {},
   "source": [
    "The following code aims to build a two layer feedforward net. It has two layers, the input dim is 30, hidden dim is 50, output dim is also 50.\n",
    "\n",
    "Find all bugs in the following code \n",
    "\n",
    "(Similar questions will be asked in the exam)\n",
    "\n",
    "```\n",
    "class MyModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().init()\n",
    "        self.nn1 = nn.Linear(in_features=30, out_features=50)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.nn2 = nn.Linear(in_features=40, out_features=50)\n",
    "        self.ac2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(input):\n",
    "        out1 = self.nn1(input)\n",
    "        out2 = self.ac1(out1)\n",
    "        out3 = self.nn2(out2)\n",
    "        out4 = self.ac2(out2)\n",
    "        return out3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc726c",
   "metadata": {},
   "source": [
    "1. init() is not correct --> \\__init__()\n",
    "2. output and input features don't match (50 to 40)\n",
    "3. forward does not call self and \n",
    "4. does not return the 4th output \n",
    "5. and the 4th output should take out3 not out2 as it's argument.\n",
    "6. MyModel needs nn.Module passed. (parent module)\n",
    "7. Import PyTorch Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbee2818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn1.weight Parameter containing:\n",
      "tensor([[-0.0252, -0.0113, -0.0681,  ...,  0.0937,  0.0899, -0.1261],\n",
      "        [ 0.1801,  0.0104, -0.0703,  ..., -0.0927,  0.0668,  0.0200],\n",
      "        [ 0.0134,  0.0467, -0.0390,  ...,  0.0947,  0.0567,  0.1776],\n",
      "        ...,\n",
      "        [ 0.0668,  0.0809, -0.1428,  ...,  0.1135, -0.0787, -0.1601],\n",
      "        [ 0.1507,  0.0383, -0.0966,  ..., -0.0182, -0.0892, -0.1333],\n",
      "        [ 0.1181,  0.0790, -0.1725,  ...,  0.0781, -0.1406,  0.0799]],\n",
      "       requires_grad=True)\n",
      "nn1.bias Parameter containing:\n",
      "tensor([-0.1347, -0.1694,  0.0566,  0.0276,  0.1646,  0.1790,  0.1711,  0.0820,\n",
      "        -0.0010, -0.0343,  0.1711,  0.1040, -0.1110,  0.1820,  0.0094, -0.0706,\n",
      "        -0.0559,  0.1480, -0.0037, -0.1410,  0.0570,  0.1741, -0.0620,  0.0479,\n",
      "        -0.0035,  0.0463, -0.1796,  0.1699, -0.1242,  0.0358, -0.0572, -0.0448,\n",
      "        -0.0193,  0.1395, -0.0862,  0.1504,  0.0694, -0.1019,  0.0653, -0.1270,\n",
      "         0.1578,  0.0710,  0.0685,  0.1083, -0.0276,  0.0866, -0.0090,  0.1375,\n",
      "        -0.0918, -0.1404], requires_grad=True)\n",
      "nn2.weight Parameter containing:\n",
      "tensor([[-0.1301,  0.0009, -0.0778,  ..., -0.0446, -0.1435,  0.1543],\n",
      "        [ 0.1408,  0.0604,  0.1559,  ...,  0.0704,  0.0187,  0.0589],\n",
      "        [-0.0546,  0.0906,  0.1504,  ...,  0.0463, -0.0603, -0.0280],\n",
      "        ...,\n",
      "        [-0.0559, -0.0122, -0.0577,  ...,  0.1491,  0.0938, -0.1307],\n",
      "        [-0.0777,  0.1533,  0.0281,  ..., -0.0858, -0.1054, -0.0649],\n",
      "        [ 0.1013, -0.0732, -0.1427,  ...,  0.0327, -0.1136, -0.0539]],\n",
      "       requires_grad=True)\n",
      "nn2.bias Parameter containing:\n",
      "tensor([ 0.1163,  0.1425,  0.0082, -0.1112,  0.1340,  0.0599,  0.0930,  0.1560,\n",
      "         0.1233,  0.1195, -0.0832, -0.1008,  0.0026, -0.1272, -0.1173,  0.0065,\n",
      "         0.0755,  0.0317,  0.1365,  0.1375,  0.1549, -0.0198,  0.0854, -0.0054,\n",
      "         0.1320,  0.0038, -0.0221,  0.1434,  0.0346,  0.0698, -0.1162, -0.0854,\n",
      "        -0.1162,  0.0495,  0.0127, -0.0812,  0.1136,  0.1141, -0.0045,  0.1559,\n",
      "         0.1361, -0.0058,  0.0507, -0.1085, -0.1537,  0.1081,  0.0416, -0.1330,\n",
      "         0.0971, -0.0410], requires_grad=True)\n",
      "MyModel(\n",
      "  (nn1): Linear(in_features=30, out_features=50, bias=True)\n",
      "  (ac1): ReLU()\n",
      "  (nn2): Linear(in_features=40, out_features=50, bias=True)\n",
      "  (ac2): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nn1 = nn.Linear(in_features=30, out_features=50)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.nn2 = nn.Linear(in_features=40, out_features=50)\n",
    "        self.ac2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out1 = self.nn1(input)\n",
    "        out2 = self.ac1(out1)\n",
    "        out3 = self.nn2(out2)\n",
    "        out4 = self.ac2(out3)\n",
    "        return out4\n",
    "    \n",
    "\n",
    "mdl = MyModel()\n",
    "for name, param in mdl.named_parameters():\n",
    "    print(name, param)\n",
    "print(mdl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
