{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77ef92d",
   "metadata": {},
   "source": [
    "# Single-layer Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e975c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1233ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim=300, out_dim=100):\n",
    "        super().__init__()\n",
    "        self.wxb = nn.Linear(in_features=in_dim, out_features=out_dim, bias=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.wxb(input)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "302fc00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100])\n"
     ]
    }
   ],
   "source": [
    "mdl = MyModel(in_dim=300, out_dim=100)\n",
    "audio = torch.randn((3, 300))\n",
    "out = mdl(audio)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f265a",
   "metadata": {},
   "source": [
    "# Multi layer Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c7315f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim=300, hidden_dim=200, out_dim=100):\n",
    "        super().__init__()\n",
    "        self.nn1 = nn.Linear(in_features=in_dim, out_features=hidden_dim)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.nn2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.ac2 = nn.Sigmoid()\n",
    "        self.nn3 = nn.Linear(in_features=hidden_dim, out_features=out_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        nn1out = self.nn1(input)\n",
    "        ann1out = self.ac1(nn1out)\n",
    "        nn2out = self.nn2(ann1out)\n",
    "        ann2out = self.ac2(nn2out)\n",
    "        nn3out = self.nn3(ann2out)\n",
    "        return nn3out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba2794bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "mymultlayer = MultiMyModel(in_dim=300, hidden_dim=200, out_dim=3)\n",
    "audio = torch.randn((4, 300))\n",
    "out = mymultlayer(audio)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b54dc3",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9df9dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "tensor(0.5243, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Binary cross entropy\n",
    "audio = torch.randn((4, 300))\n",
    "single_layer = MyModel(in_dim=300, out_dim=1)\n",
    "# don't do softmax in the model, because the loss functions expects 'logits'\n",
    "# logits means the output before applying the softmax\n",
    "out = single_layer(audio)\n",
    "label = torch.Tensor([[0, 0, 1, 1]])\n",
    "label = label.T\n",
    "print(label.shape)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# https://docs.pytorch.org/docs/stable/nn.html#loss-functions\n",
    "loss = loss_fn(out, label)\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3aa5d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor(1.2139, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# cross entropy loss\n",
    "audio = torch.randn((4, 300))\n",
    "multi_layer = MultiMyModel(in_dim=300, out_dim=3)\n",
    "# also don't apply softmax, based on cross entropy's low level \n",
    "# implementation, it also requires logits as inputs\n",
    "# link: https://github.com/pytorch/pytorch/blob/d38164a545b4a4e4e0cf73ce67173f70574890b6/torch/nn/modules/loss.py#L1194\n",
    "out = multi_layer(audio)\n",
    "label = torch.tensor([0, 1, 2, 1], dtype=int)\n",
    "print(label.shape)\n",
    "\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "ce_loss = ce_loss_fn(out, label)\n",
    "print(ce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f8802437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more functions can be found\n",
    "# https://docs.pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367b200",
   "metadata": {},
   "source": [
    "# How to do backward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "93afbfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wxb.weight torch.Size([1, 300]) None\n",
      "wxb.bias torch.Size([1]) None\n"
     ]
    }
   ],
   "source": [
    "# if we don't do backward, no graidents would be calculated\n",
    "for name, param in single_layer.named_parameters():\n",
    "    print(name, param.shape, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed546c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn1.weight torch.Size([200, 300]) tensor([[ 2.6152e-04, -1.8416e-04, -9.4570e-05,  ...,  6.1853e-04,\n",
      "          1.5009e-04, -2.5850e-04],\n",
      "        [-8.6773e-05,  4.4729e-05, -2.2361e-04,  ...,  9.8543e-05,\n",
      "         -1.5698e-05, -7.5120e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-9.3883e-04, -1.0768e-04, -2.0826e-03,  ...,  1.1355e-03,\n",
      "          3.6906e-04, -9.3242e-04],\n",
      "        [-6.3052e-05, -2.8858e-04, -5.9982e-05,  ...,  2.5686e-04,\n",
      "         -1.2360e-05, -1.6282e-04],\n",
      "        [-3.7738e-04,  5.9500e-04, -9.2371e-04,  ..., -2.2870e-06,\n",
      "         -9.0266e-05, -8.6243e-05]])\n",
      "nn1.bias torch.Size([200]) tensor([ 5.8621e-04,  1.2575e-04,  0.0000e+00, -8.6024e-04,  1.2154e-04,\n",
      "        -1.4094e-03, -1.4220e-03,  1.2874e-03, -2.5511e-04,  2.7287e-04,\n",
      "        -9.3017e-04, -1.1675e-03, -8.8709e-04, -1.5700e-03, -3.9395e-04,\n",
      "         1.7214e-03, -2.8490e-03, -2.9439e-04,  9.5461e-04, -1.7642e-03,\n",
      "         3.9491e-05,  1.2818e-03,  1.1993e-03,  8.3704e-05,  0.0000e+00,\n",
      "         3.5436e-04,  1.8632e-04, -1.9054e-03,  7.1696e-04, -8.8121e-04,\n",
      "        -1.5880e-03, -2.9148e-04,  2.1930e-04,  8.7251e-04,  9.8733e-05,\n",
      "         1.2525e-03, -1.7131e-03,  7.3566e-04,  7.6000e-04,  1.4157e-03,\n",
      "         3.5625e-04,  2.5421e-03, -1.6908e-03,  7.8086e-04,  2.8237e-05,\n",
      "         5.1839e-03,  2.0439e-03,  0.0000e+00, -8.3030e-04, -1.1165e-03,\n",
      "         4.6166e-04,  0.0000e+00, -2.1347e-03,  6.6661e-04, -1.6318e-03,\n",
      "         7.3018e-04,  5.4835e-04,  6.3633e-04,  0.0000e+00,  4.9278e-03,\n",
      "        -1.1899e-03, -1.4827e-03,  3.5023e-03,  1.3946e-03,  9.0138e-04,\n",
      "        -1.6370e-04, -5.2430e-04,  0.0000e+00, -3.3909e-04,  1.1797e-03,\n",
      "         1.3535e-03,  3.9303e-04, -3.5509e-03,  8.1739e-04, -1.8937e-04,\n",
      "         5.5864e-05, -3.6432e-05, -1.0525e-03,  0.0000e+00,  4.1215e-03,\n",
      "         1.0282e-03, -4.5873e-04,  5.3185e-03,  1.1593e-03, -5.4252e-05,\n",
      "         1.5867e-03, -4.7724e-04, -1.4407e-04, -1.0036e-03, -3.9703e-04,\n",
      "         6.3225e-04,  3.2692e-05,  0.0000e+00,  1.5740e-03,  3.1351e-04,\n",
      "        -5.1308e-04, -3.9180e-03,  2.3549e-03,  4.1540e-04, -1.6171e-03,\n",
      "        -1.0198e-03,  7.6105e-05,  2.3924e-03,  3.0304e-04, -2.4149e-03,\n",
      "        -1.2744e-04,  2.0728e-03,  0.0000e+00,  2.2695e-04, -9.4299e-06,\n",
      "         0.0000e+00, -3.9473e-04,  1.2051e-03, -6.8540e-04, -7.1973e-04,\n",
      "        -1.4850e-03, -1.8446e-03, -5.0222e-04,  9.1630e-04,  1.0632e-03,\n",
      "         1.5884e-03, -8.0113e-05, -1.6480e-04, -1.2301e-03, -2.4981e-03,\n",
      "         2.2816e-04,  5.4129e-04, -6.1044e-03,  8.1463e-04, -1.6965e-03,\n",
      "         6.0215e-04,  0.0000e+00, -3.4126e-04, -1.4329e-03,  0.0000e+00,\n",
      "         8.3872e-04,  4.6806e-03, -3.2913e-03, -1.3514e-03, -1.1042e-03,\n",
      "        -3.6150e-03,  3.2204e-04, -7.9423e-04,  1.6425e-03,  2.9963e-03,\n",
      "         0.0000e+00,  7.7385e-04,  6.1135e-04, -5.6993e-04, -1.5568e-03,\n",
      "        -3.0074e-03,  2.7499e-03,  4.7039e-03,  1.4094e-03,  1.9178e-03,\n",
      "        -1.3007e-03,  1.1485e-03,  3.9327e-04,  1.8763e-03, -6.4706e-04,\n",
      "         4.6747e-04,  1.6890e-03, -1.4508e-03,  0.0000e+00,  3.1094e-03,\n",
      "         1.4088e-03,  3.5888e-04, -1.0758e-03,  6.4864e-05,  4.0314e-04,\n",
      "         1.6606e-03,  1.0757e-03,  9.9004e-04, -1.1679e-03,  3.1487e-04,\n",
      "         0.0000e+00, -1.6943e-03, -9.1125e-04,  2.2450e-03, -1.2495e-03,\n",
      "         3.9533e-03,  9.0991e-04,  2.5185e-04, -2.4154e-03,  2.0756e-03,\n",
      "        -7.7401e-04,  3.9204e-04, -7.1405e-04, -1.2907e-03,  4.5311e-04,\n",
      "         5.4111e-04,  1.6477e-03, -2.2792e-03, -2.3000e-03, -8.9704e-04,\n",
      "         1.1166e-03, -7.2042e-04, -6.1999e-04, -4.2390e-04,  9.5432e-04])\n",
      "nn2.weight torch.Size([200, 200]) tensor([[-1.3532e-03, -8.6992e-04,  0.0000e+00,  ..., -1.0050e-03,\n",
      "         -1.1729e-03, -4.0765e-03],\n",
      "        [-1.5816e-03, -9.9185e-04,  0.0000e+00,  ..., -3.9840e-04,\n",
      "         -1.3368e-03, -4.6766e-03],\n",
      "        [ 5.5872e-04,  4.4958e-04,  0.0000e+00,  ...,  3.1460e-03,\n",
      "          6.1079e-04,  2.0062e-03],\n",
      "        ...,\n",
      "        [-1.0818e-06, -4.6342e-05,  0.0000e+00,  ..., -1.4652e-03,\n",
      "         -6.4058e-05, -1.6533e-04],\n",
      "        [ 6.1045e-04,  3.1294e-04,  0.0000e+00,  ..., -2.0685e-03,\n",
      "          4.2450e-04,  1.5640e-03],\n",
      "        [-1.1319e-03, -6.1603e-04,  0.0000e+00,  ...,  2.7448e-03,\n",
      "         -8.3521e-04, -3.0251e-03]])\n",
      "nn2.bias torch.Size([200]) tensor([-5.3604e-03, -5.6064e-03,  4.4876e-03, -1.8521e-03,  2.6951e-03,\n",
      "         7.4823e-03,  2.0332e-03, -9.9458e-04,  6.7943e-03, -4.8172e-04,\n",
      "        -3.2516e-03,  3.1919e-03, -2.6105e-03,  1.3669e-03,  2.8433e-04,\n",
      "        -4.1552e-04, -8.9105e-04, -3.8626e-03, -1.1163e-04, -1.2323e-03,\n",
      "         5.1157e-06,  1.8096e-03,  2.0095e-03, -7.0417e-04, -2.9035e-03,\n",
      "         4.0182e-03, -1.1516e-03, -1.2374e-03, -6.2543e-03,  6.3307e-03,\n",
      "         2.3656e-04, -1.8005e-03,  1.8357e-03,  3.4069e-03,  4.2981e-03,\n",
      "        -3.5617e-03,  6.2039e-03,  1.6419e-03,  1.1721e-03,  2.5706e-03,\n",
      "         4.0901e-03,  4.6769e-03, -1.3241e-03,  1.6319e-04,  7.3668e-03,\n",
      "        -8.5962e-04, -5.9092e-03,  8.9788e-04,  2.5837e-03,  3.5505e-04,\n",
      "         5.5338e-03,  6.1318e-03, -7.4888e-03,  2.5097e-03, -4.6547e-03,\n",
      "         4.3968e-03,  3.2257e-03, -1.5606e-03,  7.3306e-03,  1.5907e-03,\n",
      "        -5.5706e-03, -2.2263e-03, -8.5273e-04, -1.3446e-03,  2.1558e-03,\n",
      "        -2.9058e-04,  3.4644e-03, -7.3591e-04, -1.4628e-03, -2.2875e-03,\n",
      "        -4.5256e-03, -2.9324e-03,  5.7267e-03,  1.6864e-03, -1.7840e-03,\n",
      "        -1.0337e-03, -9.4255e-04, -1.5353e-03,  3.8388e-04, -4.7669e-03,\n",
      "         3.8948e-03, -7.5419e-04,  4.6763e-03, -2.1323e-03,  1.4791e-03,\n",
      "         5.4926e-03,  2.7654e-03,  4.3016e-03, -3.9028e-03,  2.7665e-03,\n",
      "        -3.9542e-03,  5.4058e-04,  6.8716e-03, -2.3194e-04, -5.0610e-03,\n",
      "        -2.0982e-04,  7.1436e-04,  3.9571e-03,  4.8993e-04, -9.1151e-04,\n",
      "         2.1068e-03,  6.5179e-03,  4.0344e-03, -5.0569e-03,  4.7180e-04,\n",
      "         2.3808e-03,  1.5303e-03, -2.4292e-03, -3.9408e-03, -1.0566e-03,\n",
      "         3.8496e-03,  7.8816e-04,  4.7477e-04, -5.9478e-03,  9.2295e-04,\n",
      "        -1.9556e-03,  1.1589e-03, -2.2214e-03,  6.2274e-03, -5.9140e-03,\n",
      "        -3.9130e-04, -3.2494e-03,  2.5924e-03, -1.9047e-04, -1.0991e-03,\n",
      "        -3.8140e-03, -1.1027e-03,  2.4285e-03, -4.8430e-03, -6.7484e-03,\n",
      "         3.5018e-03, -3.0638e-03,  3.7961e-04,  2.0953e-03, -6.0765e-03,\n",
      "         2.6037e-04,  4.0739e-04, -6.4619e-04, -2.6062e-03, -7.0508e-03,\n",
      "        -1.5361e-03,  2.2519e-03, -8.9636e-04,  3.1567e-03, -2.4703e-03,\n",
      "        -1.4033e-03,  1.4433e-04,  9.7218e-04, -1.3752e-03, -7.0528e-03,\n",
      "         5.8892e-03,  7.8515e-04,  7.4912e-03, -3.8397e-03,  4.5174e-03,\n",
      "        -1.4712e-03,  7.3241e-03,  6.9709e-03,  4.5142e-03,  2.2289e-03,\n",
      "         2.6638e-03,  1.3453e-03,  3.8103e-03,  6.1686e-03, -3.8823e-03,\n",
      "        -2.6011e-04, -2.9889e-04,  1.8255e-03, -3.3648e-03,  3.7585e-03,\n",
      "        -2.1828e-03, -4.4732e-03, -4.1254e-03, -6.4078e-03,  1.8488e-03,\n",
      "         4.4793e-03, -8.6139e-04, -6.9465e-04,  6.3334e-03, -4.2662e-03,\n",
      "         4.9059e-03, -4.3534e-03,  1.7183e-03,  4.7311e-03, -1.3312e-03,\n",
      "        -2.9593e-03, -2.2371e-03, -2.1012e-03,  1.0514e-05,  1.7613e-03,\n",
      "         1.5222e-03,  8.6492e-04, -1.3085e-04, -8.3334e-05,  3.9847e-03,\n",
      "         4.5777e-03,  6.3285e-04, -1.2696e-03,  2.6792e-04, -1.4054e-03])\n",
      "nn3.weight torch.Size([3, 200]) tensor([[ 7.0307e-03, -5.4395e-03,  2.0189e-02, -2.2316e-03, -8.7038e-03,\n",
      "         -1.2252e-02, -1.6306e-02,  2.5110e-02,  1.6052e-02, -6.5672e-03,\n",
      "          1.9903e-02,  6.0759e-04, -1.5485e-06,  1.2971e-02,  1.7991e-02,\n",
      "         -1.1918e-02, -1.4187e-02, -8.2898e-03,  7.6968e-03,  5.3397e-04,\n",
      "         -9.0997e-03, -5.8959e-03, -4.9379e-03,  2.1412e-03, -4.5903e-03,\n",
      "          1.3086e-02, -1.7460e-03, -1.2053e-02, -6.6759e-03, -5.0423e-03,\n",
      "         -7.7824e-03,  1.6164e-02,  1.4254e-02,  9.3628e-03, -1.5255e-02,\n",
      "         -9.6174e-04, -9.0335e-04, -1.3364e-02,  8.2745e-03,  1.7457e-02,\n",
      "         -2.4118e-03,  6.2743e-04, -2.6525e-03, -1.7121e-02,  5.8347e-03,\n",
      "         -5.1795e-03,  5.5313e-03, -1.2585e-02, -7.6011e-03, -2.2815e-02,\n",
      "         -1.4890e-02, -6.8724e-03,  3.3384e-03,  1.4792e-02, -1.6270e-02,\n",
      "          2.0421e-03, -2.4775e-02, -5.0238e-03, -1.9334e-03,  1.0269e-02,\n",
      "         -1.6008e-02, -5.7352e-03,  2.2260e-04, -1.3899e-02, -4.9749e-03,\n",
      "         -2.1711e-03,  6.8319e-03,  6.9502e-04, -2.5807e-02, -1.6200e-02,\n",
      "         -2.2191e-03, -2.9609e-03,  4.8381e-03, -1.4934e-02,  1.5861e-02,\n",
      "          1.9751e-02,  2.5574e-03, -6.3668e-03, -1.0651e-02, -1.0469e-02,\n",
      "         -6.7454e-03, -3.1478e-03,  1.3258e-02, -4.4625e-03, -1.1298e-02,\n",
      "         -9.3833e-03, -7.0910e-03,  5.3070e-03, -1.9982e-03, -1.1352e-02,\n",
      "          5.0602e-03, -7.7546e-03, -3.9335e-04,  1.8530e-02, -1.2588e-02,\n",
      "          2.1446e-03, -1.4304e-02, -7.8616e-03,  1.7205e-03, -1.2836e-02,\n",
      "          4.8450e-03, -1.5679e-02,  2.0899e-02,  9.2292e-03,  5.5151e-03,\n",
      "         -2.4249e-04, -9.2047e-03, -5.9616e-03,  2.1776e-02, -5.3585e-03,\n",
      "          2.0957e-03, -3.2973e-03,  2.4280e-03,  3.4868e-03,  4.9031e-03,\n",
      "         -7.5705e-03,  1.6737e-02,  7.5602e-03,  5.2881e-03, -3.5806e-03,\n",
      "         -1.8782e-02,  1.3315e-02,  1.4701e-02,  2.1572e-03, -2.8464e-02,\n",
      "         -9.2212e-03, -1.2240e-03,  7.3472e-03, -1.3468e-02,  5.6684e-03,\n",
      "         -1.6411e-02, -7.9052e-04,  4.5129e-03, -2.6235e-02, -3.2526e-03,\n",
      "          2.4852e-02, -2.5887e-03, -1.9961e-02,  5.3977e-03,  3.3062e-03,\n",
      "         -3.6256e-03, -6.1726e-03,  2.0555e-03, -5.5348e-03, -8.2524e-03,\n",
      "         -7.4842e-03, -6.1817e-03,  1.8354e-03, -9.9439e-03, -1.7824e-02,\n",
      "          3.6613e-03, -1.2488e-02,  7.3076e-04,  3.1377e-03,  1.5136e-02,\n",
      "         -1.3274e-02, -4.1804e-03, -5.1209e-03, -1.1432e-02, -1.2402e-02,\n",
      "         -1.8249e-02, -2.1943e-02, -1.3539e-02, -2.0859e-02, -1.6241e-02,\n",
      "          7.2519e-03,  2.2135e-03, -1.0455e-02, -3.1994e-03, -9.4006e-03,\n",
      "         -8.6470e-03,  8.3381e-04, -3.6958e-04, -1.5756e-02, -1.2316e-02,\n",
      "         -5.8650e-03,  2.1913e-03, -1.2672e-02, -1.6721e-02,  5.1614e-03,\n",
      "         -1.7837e-02,  6.1786e-03,  1.2923e-03,  7.1755e-04, -6.9907e-03,\n",
      "          6.3782e-04,  9.0342e-03,  1.0782e-02,  1.7674e-02,  7.6196e-03,\n",
      "         -1.1665e-02, -1.9366e-02, -2.3878e-03,  2.3977e-02, -5.5440e-03,\n",
      "          1.0051e-03,  1.2101e-02,  9.6835e-03,  1.9691e-02,  8.2806e-03],\n",
      "        [-1.2347e-01, -1.2841e-01, -1.3474e-01, -1.1766e-01, -1.2732e-01,\n",
      "         -1.0662e-01, -1.1369e-01, -1.4656e-01, -1.6823e-01, -1.1205e-01,\n",
      "         -1.4470e-01, -1.2078e-01, -1.2716e-01, -1.3316e-01, -1.4238e-01,\n",
      "         -9.6739e-02, -1.3137e-01, -1.4556e-01, -1.2872e-01, -1.1672e-01,\n",
      "         -1.2738e-01, -1.2414e-01, -1.2455e-01, -1.1359e-01, -1.3463e-01,\n",
      "         -1.2943e-01, -1.0856e-01, -1.3889e-01, -1.3382e-01, -1.3236e-01,\n",
      "         -1.2328e-01, -1.5874e-01, -1.3230e-01, -1.5418e-01, -1.1056e-01,\n",
      "         -9.1845e-02, -1.0661e-01, -1.1241e-01, -1.2398e-01, -1.3660e-01,\n",
      "         -1.0711e-01, -1.2926e-01, -1.4154e-01, -1.1039e-01, -1.4797e-01,\n",
      "         -1.1553e-01, -1.2801e-01, -1.2197e-01, -1.5065e-01, -9.9895e-02,\n",
      "         -1.2677e-01, -1.0072e-01, -1.2698e-01, -1.2560e-01, -1.2550e-01,\n",
      "         -1.2412e-01, -1.2223e-01, -1.2972e-01, -1.3263e-01, -1.3364e-01,\n",
      "         -1.1137e-01, -1.2475e-01, -1.2802e-01, -1.2679e-01, -1.1089e-01,\n",
      "         -1.1276e-01, -1.2220e-01, -1.0178e-01, -9.7885e-02, -1.1206e-01,\n",
      "         -1.1824e-01, -1.3021e-01, -1.3328e-01, -9.6949e-02, -1.5031e-01,\n",
      "         -1.2100e-01, -1.2616e-01, -1.0472e-01, -1.1372e-01, -1.3072e-01,\n",
      "         -1.1981e-01, -1.3211e-01, -1.1290e-01, -1.1656e-01, -1.3739e-01,\n",
      "         -1.3051e-01, -1.1425e-01, -1.3162e-01, -1.1186e-01, -1.2320e-01,\n",
      "         -1.4720e-01, -1.2662e-01, -1.4264e-01, -1.5318e-01, -9.9113e-02,\n",
      "         -1.1494e-01, -8.4221e-02, -1.2172e-01, -1.1663e-01, -1.1346e-01,\n",
      "         -1.2261e-01, -1.2283e-01, -1.4997e-01, -1.1000e-01, -1.2840e-01,\n",
      "         -1.1020e-01, -1.0914e-01, -1.1822e-01, -1.4947e-01, -1.3532e-01,\n",
      "         -1.4106e-01, -1.2413e-01, -1.1637e-01, -1.2625e-01, -1.2765e-01,\n",
      "         -1.0802e-01, -1.2120e-01, -1.4284e-01, -1.3945e-01, -1.1367e-01,\n",
      "         -1.1892e-01, -1.2723e-01, -1.2111e-01, -1.0284e-01, -1.0118e-01,\n",
      "         -1.0940e-01, -1.1672e-01, -1.2706e-01, -1.0801e-01, -1.0636e-01,\n",
      "         -1.1288e-01, -1.1267e-01, -1.3530e-01, -1.1689e-01, -1.1013e-01,\n",
      "         -1.3396e-01, -1.3064e-01, -1.1518e-01, -1.4248e-01, -1.1330e-01,\n",
      "         -1.4641e-01, -1.1943e-01, -1.2328e-01, -1.2652e-01, -1.1501e-01,\n",
      "         -1.2623e-01, -1.0662e-01, -1.2527e-01, -1.0431e-01, -1.0972e-01,\n",
      "         -1.4544e-01, -9.5900e-02, -1.5037e-01, -1.3414e-01, -1.1915e-01,\n",
      "         -1.2352e-01, -1.1919e-01, -1.1861e-01, -1.4162e-01, -9.6607e-02,\n",
      "         -1.0560e-01, -1.2014e-01, -1.0685e-01, -1.2658e-01, -1.2839e-01,\n",
      "         -1.3272e-01, -1.2080e-01, -8.8985e-02, -1.2598e-01, -9.8254e-02,\n",
      "         -1.1107e-01, -1.1631e-01, -1.1077e-01, -9.6439e-02, -1.3566e-01,\n",
      "         -1.2251e-01, -1.3017e-01, -1.0374e-01, -9.9738e-02, -1.2021e-01,\n",
      "         -1.1143e-01, -1.3421e-01, -1.0905e-01, -1.3190e-01, -1.1402e-01,\n",
      "         -1.0185e-01, -9.1569e-02, -1.2623e-01, -1.4218e-01, -1.4510e-01,\n",
      "         -1.1260e-01, -1.1487e-01, -1.3762e-01, -1.5697e-01, -1.1438e-01,\n",
      "         -1.0709e-01, -1.2923e-01, -1.4187e-01, -1.1822e-01, -1.1310e-01],\n",
      "        [ 1.1644e-01,  1.3385e-01,  1.1455e-01,  1.1989e-01,  1.3602e-01,\n",
      "          1.1887e-01,  1.3000e-01,  1.2145e-01,  1.5218e-01,  1.1862e-01,\n",
      "          1.2480e-01,  1.2017e-01,  1.2716e-01,  1.2018e-01,  1.2439e-01,\n",
      "          1.0866e-01,  1.4555e-01,  1.5385e-01,  1.2103e-01,  1.1619e-01,\n",
      "          1.3648e-01,  1.3004e-01,  1.2949e-01,  1.1144e-01,  1.3922e-01,\n",
      "          1.1635e-01,  1.1030e-01,  1.5095e-01,  1.4050e-01,  1.3741e-01,\n",
      "          1.3107e-01,  1.4258e-01,  1.1804e-01,  1.4482e-01,  1.2581e-01,\n",
      "          9.2807e-02,  1.0751e-01,  1.2577e-01,  1.1571e-01,  1.1915e-01,\n",
      "          1.0952e-01,  1.2864e-01,  1.4420e-01,  1.2751e-01,  1.4214e-01,\n",
      "          1.2071e-01,  1.2248e-01,  1.3456e-01,  1.5825e-01,  1.2271e-01,\n",
      "          1.4166e-01,  1.0759e-01,  1.2364e-01,  1.1080e-01,  1.4177e-01,\n",
      "          1.2208e-01,  1.4701e-01,  1.3475e-01,  1.3457e-01,  1.2337e-01,\n",
      "          1.2738e-01,  1.3048e-01,  1.2780e-01,  1.4069e-01,  1.1587e-01,\n",
      "          1.1493e-01,  1.1537e-01,  1.0108e-01,  1.2369e-01,  1.2826e-01,\n",
      "          1.2045e-01,  1.3318e-01,  1.2844e-01,  1.1188e-01,  1.3445e-01,\n",
      "          1.0125e-01,  1.2360e-01,  1.1109e-01,  1.2437e-01,  1.4119e-01,\n",
      "          1.2655e-01,  1.3526e-01,  9.9640e-02,  1.2102e-01,  1.4869e-01,\n",
      "          1.3989e-01,  1.2134e-01,  1.2631e-01,  1.1386e-01,  1.3455e-01,\n",
      "          1.4214e-01,  1.3438e-01,  1.4304e-01,  1.3465e-01,  1.1170e-01,\n",
      "          1.1279e-01,  9.8525e-02,  1.2958e-01,  1.1491e-01,  1.2630e-01,\n",
      "          1.1777e-01,  1.3850e-01,  1.2907e-01,  1.0077e-01,  1.2288e-01,\n",
      "          1.1044e-01,  1.1835e-01,  1.2418e-01,  1.2769e-01,  1.4068e-01,\n",
      "          1.3897e-01,  1.2743e-01,  1.1394e-01,  1.2277e-01,  1.2275e-01,\n",
      "          1.1559e-01,  1.0446e-01,  1.3528e-01,  1.3416e-01,  1.1725e-01,\n",
      "          1.3771e-01,  1.1392e-01,  1.0641e-01,  1.0068e-01,  1.2965e-01,\n",
      "          1.1862e-01,  1.1794e-01,  1.1971e-01,  1.2148e-01,  1.0069e-01,\n",
      "          1.2929e-01,  1.1346e-01,  1.3078e-01,  1.4312e-01,  1.1338e-01,\n",
      "          1.0911e-01,  1.3323e-01,  1.3514e-01,  1.3708e-01,  1.0999e-01,\n",
      "          1.5004e-01,  1.2560e-01,  1.2122e-01,  1.3206e-01,  1.2327e-01,\n",
      "          1.3371e-01,  1.1281e-01,  1.2344e-01,  1.1426e-01,  1.2754e-01,\n",
      "          1.4177e-01,  1.0839e-01,  1.4964e-01,  1.3100e-01,  1.0402e-01,\n",
      "          1.3680e-01,  1.2337e-01,  1.2373e-01,  1.5305e-01,  1.0901e-01,\n",
      "          1.2385e-01,  1.4208e-01,  1.2039e-01,  1.4744e-01,  1.4463e-01,\n",
      "          1.2547e-01,  1.1858e-01,  9.9440e-02,  1.2918e-01,  1.0765e-01,\n",
      "          1.1972e-01,  1.1548e-01,  1.1114e-01,  1.1220e-01,  1.4797e-01,\n",
      "          1.2837e-01,  1.2798e-01,  1.1641e-01,  1.1646e-01,  1.1505e-01,\n",
      "          1.2927e-01,  1.2803e-01,  1.0776e-01,  1.3118e-01,  1.2101e-01,\n",
      "          1.0121e-01,  8.2535e-02,  1.1545e-01,  1.2451e-01,  1.3748e-01,\n",
      "          1.2426e-01,  1.3423e-01,  1.4001e-01,  1.3300e-01,  1.1992e-01,\n",
      "          1.0608e-01,  1.1713e-01,  1.3218e-01,  9.8524e-02,  1.0482e-01]])\n",
      "nn3.bias torch.Size([3]) tensor([-0.0038, -0.2430,  0.2468])\n"
     ]
    }
   ],
   "source": [
    "# if we do backward, all parameter tensors would now have their gradients\n",
    "ce_loss.backward()\n",
    "for name, param in multi_layer.named_parameters():\n",
    "    print(name, param.shape, param.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
